# Copyright (c) 2019-2025, NVIDIA CORPORATION. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#  * Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#  * Neither the name of NVIDIA CORPORATION nor the names of its
#    contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

### Added
ezua:
  #Use next options in order to configure the application endpoint.
  virtualService:
    endpoint: "triton-inference-server.${DOMAIN_NAME}"
    istioGateway: "istio-system/ezaf-gateway"

### Added for virtualService
service:
  port: 8000

### Enabling loadBalancing will create traefik objects, Enabling autoscaling will create HPA objects.
tags:
  autoscaling: false
  loadBalancing: false 
  openshift: false

image:
  imageName: nvcr.io/nvidia/tritonserver:25.02-py3
  pullPolicy: IfNotPresent
  numGpus: 1

# add server args here e.g. --grpc-use-ssl, --grpc-server-certs, repository-poll-secs, etc
### Check s3 url
serverArgs:
  - '--model-repository=s3://http://local-s3-service.ezdata-system.svc.cluster.local:30000/mlflow.sy6s171r10/10/da6822d325eb41e6bd4336ce2d5d9ee2/artifacts/triton'
  # - '--log-verbose=2'

### Add access key properly, based on your PCAI environments. 
s3Configuration:
  secret_name: triton-s3-proxy
  AWS_SECRET_ACCESS_KEY: s3
  AWS_ACCESS_KEY_ID: 

# traefik:
#   ports:
#     triton-http:
#       port: 18000
#       exposedPort: 8000
#       expose: true
#       protocol: TCP
#     triton-grpc:
#       port: 18001
#       exposedPort: 8001
#       expose: true
#       protocol: TCP

autoscaling:
  minReplicas: 1
#   maxReplicas: 3
#   metrics:
#     - type: Pods
#       pods:
#         metric:
#           name: avg_time_queue_us
#         target:
#           type: AverageValue
#           averageValue: 50

# prometheus-adapter:
#  prometheus:
#    url: http://example-metrics-kube-prome-prometheus.default.svc.cluster.local
#    port: 9090
#  rules:
#    custom:
#      - seriesQuery: 'nv_inference_queue_duration_us{namespace="default",pod!=""}'
#        resources:
#          overrides:
#            namespace:
#              resource: "namespace"
#            pod:
#              resource: "pod"
#        name:
#          matches: "nv_inference_queue_duration_us"
#          as: "avg_time_queue_us"
#        metricsQuery: 'avg(delta(nv_inference_queue_duration_us{<<.LabelMatchers>>}[30s])/(1+delta(nv_inference_request_success{<<.LabelMatchers>>}[30s]))) by (<<.GroupBy>>)'
